{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import necessary libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport csv\nimport random\nfrom itertools import groupby","metadata":{"execution":{"iopub.status.busy":"2024-09-01T11:12:39.084122Z","iopub.execute_input":"2024-09-01T11:12:39.084500Z","iopub.status.idle":"2024-09-01T11:12:39.127317Z","shell.execute_reply.started":"2024-09-01T11:12:39.084471Z","shell.execute_reply":"2024-09-01T11:12:39.126110Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Define TicTacToeGame class","metadata":{}},{"cell_type":"code","source":"class TicTacToeGame():\n    def __init__(self):\n        self.state = '         '\n        self.player = 'X'\n        self.winner = None\n\n    def allowed_moves(self):\n        states = []\n        for i in range(len(self.state)):\n            if self.state[i] == ' ':\n                states.append(self.state[:i] + self.player + self.state[i+1:])\n        return states\n\n    def make_move(self, next_state):\n        if self.winner:\n            raise(Exception(\"Game already completed, cannot make another move!\"))\n        if not self.__valid_move(next_state):\n            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n                    self.state, next_state, self.player)))\n\n        self.state = next_state\n        self.winner = self.predict_winner(self.state)\n        if self.winner:\n            self.player = None\n        elif self.player == 'X':\n            self.player = 'O'\n        else:\n            self.player = 'X'\n\n    def playable(self):\n        return ( (not self.winner) and any(self.allowed_moves()) )\n\n    def predict_winner(self, state):\n        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n        winner = None\n        for line in lines:\n            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n            if line_state == 'XXX':\n                winner = 'X'\n            elif line_state == 'OOO':\n                winner = 'O'\n        return winner\n\n    def __valid_move(self, next_state):\n        allowed_moves = self.allowed_moves()\n        if any(state == next_state for state in allowed_moves):\n            return True\n        return False\n\n    def print_board(self):\n        s = self.state\n        print('     {} | {} | {} '.format(s[0],s[1],s[2]))\n        print('    -----------')\n        print('     {} | {} | {} '.format(s[3],s[4],s[5]))\n        print('    -----------')\n        print('     {} | {} | {} '.format(s[6],s[7],s[8]))","metadata":{"execution":{"iopub.status.busy":"2024-09-01T11:12:39.129505Z","iopub.execute_input":"2024-09-01T11:12:39.129967Z","iopub.status.idle":"2024-09-01T11:12:39.144638Z","shell.execute_reply.started":"2024-09-01T11:12:39.129928Z","shell.execute_reply":"2024-09-01T11:12:39.143580Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# class Agent():\n    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n        self.V = dict()\n        self.NewGame = game_class\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.value_player = value_player\n\n    def state_value(self, game_state):\n        return self.V.get(game_state, 0.0)\n\n    def learn_game(self, num_episodes=1000):\n        for episode in range(1, num_episodes + 1):\n            self.learn_from_episode()\n            if episode in [500, 1000, 5000, 10000, 20000, 30000]:\n                print(f\"After {episode} games:\")\n                demo_game_stats(self)\n\n    def learn_from_episode(self):\n        game = self.NewGame()\n        _, move = self.learn_select_move(game)\n        while move:\n            move = self.learn_from_move(game, move)\n\n    def learn_from_move(self, game, move):\n        game.make_move(move)\n        r = self.__reward(game)\n        td_target = r\n        next_state_value = 0.0\n        selected_next_move = None\n        if game.playable():\n            best_next_move, selected_next_move = self.learn_select_move(game)\n            next_state_value = self.state_value(best_next_move)\n        current_state_value = self.state_value(move)\n        td_target = r + next_state_value\n        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)\n        return selected_next_move\n\n    def learn_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            best_move = self.__argmax_V(allowed_state_values)\n        else:\n            best_move = self.__argmin_V(allowed_state_values)\n\n        selected_move = best_move\n        if random.random() < self.epsilon:\n            selected_move = self.__random_V(allowed_state_values)\n\n        return best_move, selected_move\n\n    def play_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            return self.__argmax_V(allowed_state_values)\n        else:\n            return self.__argmin_V(allowed_state_values)\n\n    def demo_game(self, verbose=False):\n        game = self.NewGame()\n        t = 0\n        while game.playable():\n            if verbose:\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n            move = self.play_select_move(game)\n            game.make_move(move)\n            t += 1\n        if verbose:\n            print(f\"\\nTurn {t}\\n\")\n            game.print_board()\n        if game.winner:\n            if verbose:\n                print(f\"\\n{game.winner} is the winner!\")\n            return game.winner\n        else:\n            if verbose:\n                print(\"\\nIt's a draw!\")\n            return '-'\n\n    def interactive_game(self):\n        while True:\n            play_game = input(\"Do you want to play a game of Tic Tac Toe? (yes/no): \").lower()\n            if play_game != 'yes':\n                print(\"Maybe next time!\")\n                return\n            \n            while True:\n                agent_player = input(\"Do you want to play as 'X' or 'O'? \").upper()\n                if agent_player == 'X' or agent_player == 'O':\n                    break\n                else:\n                    print(\"Invalid choice. Please choose 'X' or 'O'.\")\n\n            while True:\n                game = self.NewGame()\n                human_player = agent_player\n                if agent_player == 'X':\n                    agent_player = 'O'\n                else:\n                    agent_player = 'X'\n\n                t = 0\n                while game.playable():\n                    print(f\"\\nTurn {t}\\n\")\n                    game.print_board()\n                    if game.player == agent_player:\n                        move = self.play_select_move(game)\n                        game.make_move(move)\n                    else:\n                        move = self.__request_human_move(game)\n                        game.make_move(move)\n                    t += 1\n\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n\n                if game.winner:\n                    print(f\"\\n{game.winner} is the winner!\")\n                else:\n                    print(\"\\nIt's a draw!\")\n\n                break\n\n        print(\"Thanks for playing!\")\n        \n    def round_V(self):\n        for k in self.V.keys():\n            self.V[k] = round(self.V[k], 1)\n\n    def save_v_table(self):\n        with open('state_values.csv', 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['State', 'Value'])\n            all_states = list(self.V.keys())\n            all_states.sort()\n            for state in all_states:\n                writer.writerow([state, self.V[state]])\n\n    def __state_values(self, game_states):\n        return {state: self.state_value(state) for state in game_states}\n\n    def __argmax_V(self, state_values):\n        max_V = max(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n        return chosen_state\n\n    def __argmin_V(self, state_values):\n        min_V = min(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n        return chosen_state\n\n    def __random_V(self, state_values):\n        return random.choice(list(state_values.keys()))\n\n    def __reward(self, game):\n        if game.winner == self.value_player:\n            return 1.0\n        elif game.winner:\n            return -1.0\n        else:\n            return 0.0\n\n    def __request_human_move(self, game):\n        allowed_moves = [i + 1 for i in range(9) if game.state[i] == ' ']\n        human_move = None\n        while not human_move:\n            idx = int(input(f'Choose move for {game.player}, from {allowed_moves}: '))\n            if idx in allowed_moves:\n                human_move = game.state[:idx - 1] + game.player + game.state[idx:]\n        return human_move\n\ndef demo_game_stats(agent):\n    results = [agent.demo_game() for _ in range(10000)]\n    game_stats = {k: results.count(k) / 100 for k in ['X', 'O', '-']}\n    print(f\"    Percentage results: {game_stats}\")Define Agent class","metadata":{}},{"cell_type":"code","source":"class Agent():\n    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n        self.V = dict()\n        self.NewGame = game_class\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.value_player = value_player\n\n    def state_value(self, game_state):\n        return self.V.get(game_state, 0.0)\n\n    def learn_game(self, num_episodes=1000):\n        for episode in range(1, num_episodes + 1):\n            self.learn_from_episode()\n            if episode in [500, 1000, 5000, 10000, 20000, 30000]:\n                print(f\"After {episode} games:\")\n                demo_game_stats(self)\n\n    def learn_from_episode(self):\n        game = self.NewGame()\n        _, move = self.learn_select_move(game)\n        while move:\n            move = self.learn_from_move(game, move)\n\n    def learn_from_move(self, game, move):\n        game.make_move(move)\n        r = self.__reward(game)\n        td_target = r\n        next_state_value = 0.0\n        selected_next_move = None\n        if game.playable():\n            best_next_move, selected_next_move = self.learn_select_move(game)\n            next_state_value = self.state_value(best_next_move)\n        current_state_value = self.state_value(move)\n        td_target = r + next_state_value\n        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)\n        return selected_next_move\n\n    def learn_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            best_move = self.__argmax_V(allowed_state_values)\n        else:\n            best_move = self.__argmin_V(allowed_state_values)\n\n        selected_move = best_move\n        if random.random() < self.epsilon:\n            selected_move = self.__random_V(allowed_state_values)\n\n        return best_move, selected_move\n\n    def play_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            return self.__argmax_V(allowed_state_values)\n        else:\n            return self.__argmin_V(allowed_state_values)\n\n    def demo_game(self, verbose=False):\n        game = self.NewGame()\n        t = 0\n        while game.playable():\n            if verbose:\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n            move = self.play_select_move(game)\n            game.make_move(move)\n            t += 1\n        if verbose:\n            print(f\"\\nTurn {t}\\n\")\n            game.print_board()\n        if game.winner:\n            if verbose:\n                print(f\"\\n{game.winner} is the winner!\")\n            return game.winner\n        else:\n            if verbose:\n                print(\"\\nIt's a draw!\")\n            return '-'\n\n    def interactive_game(self):\n        while True:\n            play_game = input(\"Do you want to play a game of Tic Tac Toe? (yes/no): \").lower()\n            if play_game != 'yes':\n                print(\"Maybe next time!\")\n                return\n            \n            while True:\n                agent_player = input(\"Do you want to play as 'X' or 'O'? \").upper()\n                if agent_player == 'X' or agent_player == 'O':\n                    break\n                else:\n                    print(\"Invalid choice. Please choose 'X' or 'O'.\")\n\n            while True:\n                game = self.NewGame()\n                human_player = agent_player\n                if agent_player == 'X':\n                    agent_player = 'O'\n                else:\n                    agent_player = 'X'\n\n                t = 0\n                while game.playable():\n                    print(f\"\\nTurn {t}\\n\")\n                    game.print_board()\n                    if game.player == agent_player:\n                        move = self.play_select_move(game)\n                        game.make_move(move)\n                    else:\n                        move = self.__request_human_move(game)\n                        game.make_move(move)\n                    t += 1\n\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n\n                if game.winner:\n                    print(f\"\\n{game.winner} is the winner!\")\n                else:\n                    print(\"\\nIt's a draw!\")\n\n                break\n\n        print(\"Thanks for playing!\")\n        \n    def round_V(self):\n        for k in self.V.keys():\n            self.V[k] = round(self.V[k], 1)\n\n    def save_v_table(self):\n        with open('state_values.csv', 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['State', 'Value'])\n            all_states = list(self.V.keys())\n            all_states.sort()\n            for state in all_states:\n                writer.writerow([state, self.V[state]])\n\n    def __state_values(self, game_states):\n        return {state: self.state_value(state) for state in game_states}\n\n    def __argmax_V(self, state_values):\n        max_V = max(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n        return chosen_state\n\n    def __argmin_V(self, state_values):\n        min_V = min(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n        return chosen_state\n\n    def __random_V(self, state_values):\n        return random.choice(list(state_values.keys()))\n\n    def __reward(self, game):\n        if game.winner == self.value_player:\n            return 1.0\n        elif game.winner:\n            return -1.0\n        else:\n            return 0.0\n\n    def __request_human_move(self, game):\n        allowed_moves = [i + 1 for i in range(9) if game.state[i] == ' ']\n        human_move = None\n        while not human_move:\n            idx = int(input(f'Choose move for {game.player}, from {allowed_moves}: '))\n            if idx in allowed_moves:\n                human_move = game.state[:idx - 1] + game.player + game.state[idx:]\n        return human_move\n\ndef demo_game_stats(agent):\n    results = [agent.demo_game() for _ in range(10000)]\n    game_stats = {k: results.count(k) / 100 for k in ['X', 'O', '-']}\n    print(f\"    Percentage results: {game_stats}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T11:12:47.655455Z","iopub.execute_input":"2024-09-01T11:12:47.655849Z","iopub.status.idle":"2024-09-01T11:12:47.683801Z","shell.execute_reply.started":"2024-09-01T11:12:47.655819Z","shell.execute_reply":"2024-09-01T11:12:47.682290Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Main training and execution block","metadata":{}},{"cell_type":"code","source":"if __name__ == '__main__':\n    agent = Agent(TicTacToeGame, epsilon=0.1, alpha=1.0)\n    print(\"Training the agent...\")\n    agent.learn_game(30000)\n    print(\"Training completed.\")\n\n    while True:\n        agent.interactive_game()\n        break\n    print(\"Thanks for playing\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T11:12:57.678088Z","iopub.execute_input":"2024-09-01T11:12:57.678436Z","iopub.status.idle":"2024-09-01T11:17:07.069324Z","shell.execute_reply.started":"2024-09-01T11:12:57.678411Z","shell.execute_reply":"2024-09-01T11:17:07.068245Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training the agent...\nAfter 500 games:\n    Percentage results: {'X': 58.42, 'O': 30.76, '-': 10.82}\nAfter 1000 games:\n    Percentage results: {'X': 56.21, 'O': 32.52, '-': 11.27}\nAfter 5000 games:\n    Percentage results: {'X': 18.69, 'O': 6.88, '-': 74.43}\nAfter 10000 games:\n    Percentage results: {'X': 1.8, 'O': 0.9, '-': 97.3}\nAfter 20000 games:\n    Percentage results: {'X': 0.05, 'O': 0.01, '-': 99.94}\nAfter 30000 games:\n    Percentage results: {'X': 0.0, 'O': 0.0, '-': 100.0}\nTraining completed.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  yes\nDo you want to play as 'X' or 'O'?  X\n"},{"name":"stdout","text":"\nTurn 0\n\n       |   |   \n    -----------\n       |   |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [1, 2, 3, 4, 5, 6, 7, 8, 9]:  3\n"},{"name":"stdout","text":"\nTurn 1\n\n       |   | X \n    -----------\n       |   |   \n    -----------\n       |   |   \n\nTurn 2\n\n       |   | X \n    -----------\n       | O |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [1, 2, 4, 6, 7, 8, 9]:  9\n"},{"name":"stdout","text":"\nTurn 3\n\n       |   | X \n    -----------\n       | O |   \n    -----------\n       |   | X \n\nTurn 4\n\n       |   | X \n    -----------\n       | O | O \n    -----------\n       |   | X \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [1, 2, 4, 7, 8]:  4\n"},{"name":"stdout","text":"\nTurn 5\n\n       |   | X \n    -----------\n     X | O | O \n    -----------\n       |   | X \n\nTurn 6\n\n     O |   | X \n    -----------\n     X | O | O \n    -----------\n       |   | X \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [2, 7, 8]:  7\n"},{"name":"stdout","text":"\nTurn 7\n\n     O |   | X \n    -----------\n     X | O | O \n    -----------\n     X |   | X \n\nTurn 8\n\n     O |   | X \n    -----------\n     X | O | O \n    -----------\n     X | O | X \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [2]:  2\n"},{"name":"stdout","text":"\nTurn 9\n\n     O | X | X \n    -----------\n     X | O | O \n    -----------\n     X | O | X \n\nIt's a draw!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  yes\nDo you want to play as 'X' or 'O'?  O\n"},{"name":"stdout","text":"\nTurn 0\n\n       |   |   \n    -----------\n       |   |   \n    -----------\n       |   |   \n\nTurn 1\n\n       |   |   \n    -----------\n     X |   |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [1, 2, 3, 5, 6, 7, 8, 9]:  7\n"},{"name":"stdout","text":"\nTurn 2\n\n       |   |   \n    -----------\n     X |   |   \n    -----------\n     O |   |   \n\nTurn 3\n\n       |   |   \n    -----------\n     X | X |   \n    -----------\n     O |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [1, 2, 3, 6, 8, 9]:  6\n"},{"name":"stdout","text":"\nTurn 4\n\n       |   |   \n    -----------\n     X | X | O \n    -----------\n     O |   |   \n\nTurn 5\n\n       |   | X \n    -----------\n     X | X | O \n    -----------\n     O |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [1, 2, 8, 9]:  1\n"},{"name":"stdout","text":"\nTurn 6\n\n     O |   | X \n    -----------\n     X | X | O \n    -----------\n     O |   |   \n\nTurn 7\n\n     O | X | X \n    -----------\n     X | X | O \n    -----------\n     O |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [8, 9]:  8\n"},{"name":"stdout","text":"\nTurn 8\n\n     O | X | X \n    -----------\n     X | X | O \n    -----------\n     O | O |   \n\nTurn 9\n\n     O | X | X \n    -----------\n     X | X | O \n    -----------\n     O | O | X \n\nIt's a draw!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  yes\nDo you want to play as 'X' or 'O'?  X\n"},{"name":"stdout","text":"\nTurn 0\n\n       |   |   \n    -----------\n       |   |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [1, 2, 3, 4, 5, 6, 7, 8, 9]:  3\n"},{"name":"stdout","text":"\nTurn 1\n\n       |   | X \n    -----------\n       |   |   \n    -----------\n       |   |   \n\nTurn 2\n\n       |   | X \n    -----------\n       | O |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [1, 2, 4, 6, 7, 8, 9]:  1\n"},{"name":"stdout","text":"\nTurn 3\n\n     X |   | X \n    -----------\n       | O |   \n    -----------\n       |   |   \n\nTurn 4\n\n     X | O | X \n    -----------\n       | O |   \n    -----------\n       |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [4, 6, 7, 8, 9]:  8\n"},{"name":"stdout","text":"\nTurn 5\n\n     X | O | X \n    -----------\n       | O |   \n    -----------\n       | X |   \n\nTurn 6\n\n     X | O | X \n    -----------\n       | O |   \n    -----------\n     O | X |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [4, 6, 9]:  4\n"},{"name":"stdout","text":"\nTurn 7\n\n     X | O | X \n    -----------\n     X | O |   \n    -----------\n     O | X |   \n\nTurn 8\n\n     X | O | X \n    -----------\n     X | O | O \n    -----------\n     O | X |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for X, from [9]:  9\n"},{"name":"stdout","text":"\nTurn 9\n\n     X | O | X \n    -----------\n     X | O | O \n    -----------\n     O | X | X \n\nIt's a draw!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  yes\nDo you want to play as 'X' or 'O'?  O\n"},{"name":"stdout","text":"\nTurn 0\n\n       |   |   \n    -----------\n       |   |   \n    -----------\n       |   |   \n\nTurn 1\n\n       |   |   \n    -----------\n       |   |   \n    -----------\n     X |   |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [1, 2, 3, 4, 5, 6, 8, 9]:  1\n"},{"name":"stdout","text":"\nTurn 2\n\n     O |   |   \n    -----------\n       |   |   \n    -----------\n     X |   |   \n\nTurn 3\n\n     O |   |   \n    -----------\n       |   |   \n    -----------\n     X | X |   \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Choose move for O, from [2, 3, 4, 5, 6, 9]:  2\n"},{"name":"stdout","text":"\nTurn 4\n\n     O | O |   \n    -----------\n       |   |   \n    -----------\n     X | X |   \n\nTurn 5\n\n     O | O |   \n    -----------\n       |   |   \n    -----------\n     X | X | X \n\nX is the winner!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  no\n"},{"name":"stdout","text":"Maybe next time!\nThanks for playing\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Summary of Tic Tac Toe Game and Reinforcement Learning Agent**\n\n#### 1. 'TicTacToeGame' Class\n- **Initialization**: Initializes a Tic Tac Toe game with an empty board (\"         \"), sets the starting player as 'X', and initializes 'winner' as 'None'.\n- **Allowed Moves**: Returns a list of all possible board states after the current player makes a move.\n- **Make Move**: Updates the game state to the next state provided it's a valid move.\n- **Playable**: Checks if the game is still ongoing (no winner yet and there are allowed moves).\n- **Predict Winner**: Determines if there is a winner based on the current board state.\n- **Print Board**: Prints the current state of the board in a formatted manner.\n\n#### 2. 'Agent' Class\n- **Initialization**: Initializes the reinforcement learning agent with parameters like 'epsilon' (exploration rate), 'alpha' (learning rate), and the player it will optimize for ('value_player').\n- **State Value**: Returns the estimated value (Q-value) of a given game state.\n- **Learn Game**: Trains the agent by playing multiple episodes of Tic Tac Toe.\n- **Learn From Episode**: Plays a single episode of Tic Tac Toe and learns from it.\n- **Learn From Move**: Updates the Q-value of the current state based on the reward received and the predicted future rewards.\n- **Learn Select Move**: Selects the next move based on the current policy (exploitation vs exploration).\n- **Play Select Move**: Selects the next move during gameplay based on the learned Q-values.\n- **Demo Game**: Plays a complete game of Tic Tac Toe either silently or with verbose output.\n- **Interactive Game**: Allows the user to interactively play Tic Tac Toe against the trained agent.\n- **Round V**: Rounds all Q-values to one decimal place.\n- **Save V Table**: Saves the learned Q-values to a CSV file.\n- **Private Helper Methods**:\n  - '__state_values': Calculates Q-values for all possible moves.\n  - '__argmax_V', '__argmin_V', '__random_V': Helper methods for selecting moves based on Q-values.\n  - '__reward': Computes the reward for the agent based on the game outcome.\n  - '__request_human_move': Prompts the user for a valid move during interactive gameplay.\n\n#### 3. 'demo_game_stats' Function\n- **Demo Game Stats**: Runs multiple Tic Tac Toe games to gather statistics on win/draw percentages for 'X', 'O', and draws.\n\n#### 4. Main Execution Block\n- Initializes an instance of 'Agent' with 'TicTacToeGame'.\n- Trains the agent ('learn_game') over 30,000 episodes.\n- Offers the user the option to play interactive games against the trained agent.\n\n### Functionality Overview\n- **Training**: The agent learns to play Tic Tac Toe by playing against itself using reinforcement learning (Q-learning).\n- **Interactive Gameplay**: Users can choose to play Tic Tac Toe against the trained agent, selecting 'X' or 'O'.\n- **Statistics**: After training, the code provides statistics on game outcomes (win/draw percentages).\n\n### Usage\n- Copy the entire code into a Jupyter Notebook or Python script.\n- Run the script to train the agent and interactively play Tic Tac Toe.\n- Modify parameters such as \"epsilon\", 'alpha', or the number of training episodes to experiment with different learning behaviors.\n\nThis setup encapsulates a complete implementation of Tic Tac Toe with a reinforcement learning agent, demonstrating both training and interactive gameplay capabilities. Adjustments can be made for further customization or integration into larger projects.","metadata":{}},{"cell_type":"markdown","source":"# **Here is the full code in a single snippet**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport csv\nimport random\nfrom itertools import groupby\n\nclass TicTacToeGame():\n    def __init__(self):\n        self.state = '         '\n        self.player = 'X'\n        self.winner = None\n\n    def allowed_moves(self):\n        states = []\n        for i in range(len(self.state)):\n            if self.state[i] == ' ':\n                states.append(self.state[:i] + self.player + self.state[i+1:])\n        return states\n\n    def make_move(self, next_state):\n        if self.winner:\n            raise(Exception(\"Game already completed, cannot make another move!\"))\n        if not self.__valid_move(next_state):\n            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n                    self.state, next_state, self.player)))\n\n        self.state = next_state\n        self.winner = self.predict_winner(self.state)\n        if self.winner:\n            self.player = None\n        elif self.player == 'X':\n            self.player = 'O'\n        else:\n            self.player = 'X'\n\n    def playable(self):\n        return ( (not self.winner) and any(self.allowed_moves()) )\n\n    def predict_winner(self, state):\n        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n        winner = None\n        for line in lines:\n            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n            if line_state == 'XXX':\n                winner = 'X'\n            elif line_state == 'OOO':\n                winner = 'O'\n        return winner\n\n    def __valid_move(self, next_state):\n        allowed_moves = self.allowed_moves()\n        if any(state == next_state for state in allowed_moves):\n            return True\n        return False\n\n    def print_board(self):\n        s = self.state\n        print('     {} | {} | {} '.format(s[0],s[1],s[2]))\n        print('    -----------')\n        print('     {} | {} | {} '.format(s[3],s[4],s[5]))\n        print('    -----------')\n        print('     {} | {} | {} '.format(s[6],s[7],s[8]))\n\n\nclass Agent():\n    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n        self.V = dict()\n        self.NewGame = game_class\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.value_player = value_player\n\n    def state_value(self, game_state):\n        return self.V.get(game_state, 0.0)\n\n    def learn_game(self, num_episodes=1000):\n        for episode in range(1, num_episodes + 1):\n            self.learn_from_episode()\n            if episode in [500, 1000, 5000, 10000, 20000, 30000]:\n                print(f\"After {episode} games:\")\n                demo_game_stats(self)\n\n    def learn_from_episode(self):\n        game = self.NewGame()\n        _, move = self.learn_select_move(game)\n        while move:\n            move = self.learn_from_move(game, move)\n\n    def learn_from_move(self, game, move):\n        game.make_move(move)\n        r = self.__reward(game)\n        td_target = r\n        next_state_value = 0.0\n        selected_next_move = None\n        if game.playable():\n            best_next_move, selected_next_move = self.learn_select_move(game)\n            next_state_value = self.state_value(best_next_move)\n        current_state_value = self.state_value(move)\n        td_target = r + next_state_value\n        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)\n        return selected_next_move\n\n    def learn_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            best_move = self.__argmax_V(allowed_state_values)\n        else:\n            best_move = self.__argmin_V(allowed_state_values)\n\n        selected_move = best_move\n        if random.random() < self.epsilon:\n            selected_move = self.__random_V(allowed_state_values)\n\n        return best_move, selected_move\n\n    def play_select_move(self, game):\n        allowed_state_values = self.__state_values(game.allowed_moves())\n        if game.player == self.value_player:\n            return self.__argmax_V(allowed_state_values)\n        else:\n            return self.__argmin_V(allowed_state_values)\n\n    def demo_game(self, verbose=False):\n        game = self.NewGame()\n        t = 0\n        while game.playable():\n            if verbose:\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n            move = self.play_select_move(game)\n            game.make_move(move)\n            t += 1\n        if verbose:\n            print(f\"\\nTurn {t}\\n\")\n            game.print_board()\n        if game.winner:\n            if verbose:\n                print(f\"\\n{game.winner} is the winner!\")\n            return game.winner\n        else:\n            if verbose:\n                print(\"\\nIt's a draw!\")\n            return '-'\n\n    def interactive_game(self):\n        while True:\n            play_game = input(\"Do you want to play a game of Tic Tac Toe? (yes/no): \").lower()\n            if play_game != 'yes':\n                print(\"Maybe next time!\")\n                return\n            \n            while True:\n                agent_player = input(\"Do you want to play as 'X' or 'O'? \").upper()\n                if agent_player == 'X' or agent_player == 'O':\n                    break\n                else:\n                    print(\"Invalid choice. Please choose 'X' or 'O'.\")\n\n            while True:\n                game = self.NewGame()\n                human_player = agent_player\n                if agent_player == 'X':\n                    agent_player = 'O'\n                else:\n                    agent_player = 'X'\n\n                t = 0\n                while game.playable():\n                    print(f\"\\nTurn {t}\\n\")\n                    game.print_board()\n                    if game.player == agent_player:\n                        move = self.play_select_move(game)\n                        game.make_move(move)\n                    else:\n                        move = self.__request_human_move(game)\n                        game.make_move(move)\n                    t += 1\n\n                print(f\"\\nTurn {t}\\n\")\n                game.print_board()\n\n                if game.winner:\n                    print(f\"\\n{game.winner} is the winner!\")\n                else:\n                    print(\"\\nIt's a draw!\")\n\n                break\n\n        print(\"Thanks for playing!\")\n        \n    def round_V(self):\n        for k in self.V.keys():\n            self.V[k] = round(self.V[k], 1)\n\n    def save_v_table(self):\n        with open('state_values.csv', 'w', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['State', 'Value'])\n            all_states = list(self.V.keys())\n            all_states.sort()\n            for state in all_states:\n                writer.writerow([state, self.V[state]])\n\n    def __state_values(self, game_states):\n        return {state: self.state_value(state) for state in game_states}\n\n    def __argmax_V(self, state_values):\n        max_V = max(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n        return chosen_state\n\n    def __argmin_V(self, state_values):\n        min_V = min(state_values.values())\n        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n        return chosen_state\n\n    def __random_V(self, state_values):\n        return random.choice(list(state_values.keys()))\n\n    def __reward(self, game):\n        if game.winner == self.value_player:\n            return 1.0\n        elif game.winner:\n            return -1.0\n        else:\n            return 0.0\n\n    def __request_human_move(self, game):\n        allowed_moves = [i + 1 for i in range(9) if game.state[i] == ' ']\n        human_move = None\n        while not human_move:\n            idx = int(input(f'Choose move for {game.player}, from {allowed_moves}: '))\n            if idx in allowed_moves:\n                human_move = game.state[:idx - 1] + game.player + game.state[idx:]\n        return human_move\n\ndef demo_game_stats(agent):\n    results = [agent.demo_game() for _ in range(10000)]\n    game_stats = {k: results.count(k) / 100 for k in ['X', 'O', '-']}\n    print(f\"    Percentage results: {game_stats}\")\n\nif __name__ == '__main__':\n    agent = Agent(TicTacToeGame, epsilon=0.1, alpha=1.0)\n    print(\"Training the agent...\")\n    agent.learn_game(30000)\n    print(\"Training completed.\")\n\n    while True:\n        agent.interactive_game()\n        break\n    print(\"Thanks for playing\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T11:17:37.456286Z","iopub.execute_input":"2024-09-01T11:17:37.456669Z","iopub.status.idle":"2024-09-01T11:18:40.240078Z","shell.execute_reply.started":"2024-09-01T11:17:37.456637Z","shell.execute_reply":"2024-09-01T11:18:40.239048Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Training the agent...\nAfter 500 games:\n    Percentage results: {'X': 59.08, 'O': 30.15, '-': 10.77}\nAfter 1000 games:\n    Percentage results: {'X': 57.17, 'O': 32.0, '-': 10.83}\nAfter 5000 games:\n    Percentage results: {'X': 18.24, 'O': 7.74, '-': 74.02}\nAfter 10000 games:\n    Percentage results: {'X': 1.99, 'O': 0.47, '-': 97.54}\nAfter 20000 games:\n    Percentage results: {'X': 0.0, 'O': 0.0, '-': 100.0}\nAfter 30000 games:\n    Percentage results: {'X': 0.0, 'O': 0.0, '-': 100.0}\nTraining completed.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Do you want to play a game of Tic Tac Toe? (yes/no):  no\n"},{"name":"stdout","text":"Maybe next time!\nThanks for playing\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}